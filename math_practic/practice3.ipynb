{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое занятие 3. Наивный байесовский классификатор\n",
    "<br><br><br><br>\n",
    "__Аксентьев Артем (akseart@ya.ru)__\n",
    "\n",
    "__Ксемидов Борис (nstalker.anonim@yandex.ru)__\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from numpy import log\n",
    "from pandas import read_csv\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка текста:\n",
    "1. Приведение к нижнему регистру\n",
    "2. Токенизация\n",
    "3. Удаление стоп-слов\n",
    "4. Удаление пунктуации\n",
    "5. Фильтрация слов по частоте/длине/регулярному выражению\n",
    "6. Лемматизация или стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бегал', tag=OpencorporaTag('VERB,impf,intr masc,sing,past,indc'), normal_form='бегать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'бегал', 15, 7),))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"бегал\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бегать'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"бегал\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бегу', tag=OpencorporaTag('NOUN,inan,masc,Sgtm sing,loc2'), normal_form='бег', score=0.5, methods_stack=((DictionaryAnalyzer(), 'бегу', 184, 6),)),\n",
       " Parse(word='бегу', tag=OpencorporaTag('NOUN,inan,masc,Sgtm sing,datv'), normal_form='бег', score=0.25, methods_stack=((DictionaryAnalyzer(), 'бегу', 184, 2),)),\n",
       " Parse(word='бегу', tag=OpencorporaTag('VERB,perf,intr sing,1per,futr,indc'), normal_form='бежать', score=0.125, methods_stack=((DictionaryAnalyzer(), 'бегу', 392, 5),)),\n",
       " Parse(word='бегу', tag=OpencorporaTag('VERB,impf,intr sing,1per,pres,indc'), normal_form='бежать', score=0.125, methods_stack=((DictionaryAnalyzer(), 'бегу', 392, 43),))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"бегу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бега'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stemmer.stem(\"бегал\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бег'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"бегу\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм предобработки:\n",
    "1. Приводим всё к нижнему регистру.\n",
    "2. Удаляем из текста всё, кроме букв латиницы и кириллицы, цифр и пробелов.\n",
    "3. Токенизируем по пробелам.\n",
    "4. Фильтруем получившийся список токенов по стоп-словам.\n",
    "5. Лемматизируем токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, morph: pymorphy2.MorphAnalyzer,\n",
    "               stop_words: list = stopwords.words('english')):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9а-яё\\s]', ' ', text).split(' ')\n",
    "    text = list(filter(None, text))\n",
    "    text = [token for token in text if token not in stop_words]\n",
    "\n",
    "    text_normalize = []\n",
    "    for word in text:\n",
    "        text_normalize.append(morph.parse(word)[0].normal_form)\n",
    "\n",
    "    return text_normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['свет', 'сделать', 'вывод', 'знать', 'взяться', 'дело']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\")\n",
    "\n",
    "clean_text(\n",
    "    (\"Нет ничего на свете, из чего нельзя было бы сделать вывод.\"\n",
    "     \"Надо только знать, как взяться за дело.\"), morph, stop_words=stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [clean_text(text, morph) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['алиса', 'надо', 'сказать', 'частенько', 'давать', 'себя', 'очень', 'разумный', 'совет', 'но', 'довольно', 'редко', 'следовать', 'они'], ['впрочем', 'ведь', 'раз', 'некого', 'ответить', 'то', 'не', 'всё', 'ли', 'равно', 'о', 'что', 'спрашивать']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_texts = clean_texts(\n",
    "    [\n",
    "        \"Алиса, надо сказать, частенько давала себе очень разумные советы, но довольно редко следовала им.\",\n",
    "        \"Впрочем, ведь раз некому ответить, то не всё ли равно, о чём спрашивать?\"\n",
    "    ]\n",
    ")\n",
    "print(cleaned_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условная вероятность:\n",
    "\n",
    "$P(AB) = P(A|B) P(B) = P(B|A) P(A)$\n",
    "\n",
    "$P(A|B) = \\frac{P(AB)}{P(B)} = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "\n",
    "Теорема Байеса (вероятность события $А$ при наступлении события $B$ или апостериорная вероятность):\n",
    "\n",
    "$P(A|B) = \\frac{P(A) P(B|A)}{P(B)}$\n",
    "\n",
    "Пусть нам дан вектор признаков $x = (x_1, x_2, x_3, ..., x_n)$ и некоторый набор классов $C_1, C_2, ..., C_k$. Тогда используя теорему Байеса можно записать:\n",
    "\n",
    "$P(C_k | x) = \\frac{P(C_k) P(x | C_k)}{P(x)}$\n",
    "\n",
    "Таким образом, мы можем получить вероятности отнесения конкретного объекта к каждому классу и выбрать наиболее вероятный.\n",
    "\n",
    "$P(C_k | x_1, x_2, x_3, ..., x_n) = P(C_k) P(x_1 | C_k) P(x_2 | C_k)\\ ...\\ P(x_n | C_k)$\n",
    "\n",
    "Тогда задача сводится к\n",
    "\n",
    "$y = \\arg \\max\\limits_{C_k} P(C_k) P(x_1 | C_k) P(x_2 | C_k)\\ ...\\ P(x_n | C_k) = \\arg \\max\\limits_{C_k} P(C_k)  ∏\\limits_n p(x_i | C_k)$ \n",
    "\n",
    "Для облегчения математических операций прологарифмируем задачу (и для большей \"числовой\" стабильности):\n",
    "\n",
    "$y = \\arg \\max\\limits_{C_k} P(C_k)  ∏\\limits_n p(x_i | C_k) = \\arg \\max\\limits_{C_k} \\log P(C_k) \\sum\\limits_n \\log(p(x_i|C_k))$\n",
    "\n",
    "**Примечание:** сделать нам это позволяет монотонность логарифма (всё ещё ищем максимум) и свойство логарифма: $\\log (a b) = \\log a + \\log b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В полиномиальном наивном баейсовском классификаторе оценки вероятностей $P(x_i|C_k)$ считаются иначе:\n",
    "\n",
    "$P(x_i|C_k) = \\frac{N_{C_k i} + \\alpha}{N_{C_k} + \\alpha n}$, где\n",
    "- $\\alpha$ - коэффициент сглаживания;\n",
    "- $N_{C_k}$ - количество встреченных объектов класса $C_k$;\n",
    "- $N_{C_k i}$ - количество вхождений i-го слова в объектах класса $C_k$;\n",
    "- $n$ - размер словаря или кол-во признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм обучения**:\n",
    "1. Считаем количество объектов каждого класса ($N_{C_k}$), количества каждого слова для каждого класса ($N_{C_k i}$) и составляем словарь слов.\n",
    "2. Считаем логарифмы оценок вероятностей каждого класса ($\\log(P(C_k)) = \\log(\\frac{N_{C_k}}{n})$, где $n$ - размер датасета, а $N_{C_k}$ - количество встреченных объектов класса $C_k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм предсказания**:\n",
    "- Для каждого объекта выполняем:\n",
    "    - для каждого класса, перебирая слова ($x_i$), вычисляем $\\log P(C_k) \\sum\\limits_n \\log(p(x_i|C_k))$ и выбираем класс с максимальный значением;\n",
    "    - сохраняем предсказание для текущего объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Bayes(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.vocab = set()\n",
    "        # Счётчик классов в данных\n",
    "        self.classes = defaultdict(lambda: 0)\n",
    "        # Счётчик для каждого слова при каждом классе\n",
    "        self.feats_counts = defaultdict(lambda: 0)\n",
    "        # Прологарифмированные оценки вероятностей классов\n",
    "        self.log_classes_freq = defaultdict(lambda: 0)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for feats, label in zip(x, y):\n",
    "            self.classes[label] += 1\n",
    "            for feat, count in Counter(feats).items():\n",
    "                if feat not in self.vocab:\n",
    "                    self.vocab.add(feat)\n",
    "                self.feats_counts[label, feat] += count\n",
    "\n",
    "        n = len(x)\n",
    "        for c in self.classes:\n",
    "            self.log_classes_freq[c] = np.log(self.classes[c] / n)\n",
    "\n",
    "    def predict(self, x):\n",
    "        preds = []\n",
    "        for feats in x:\n",
    "            classes_scores = {c: 0 for c in self.classes.keys()}\n",
    "            for word in Counter(feats).keys():\n",
    "                if word not in self.vocab:\n",
    "                    continue\n",
    "\n",
    "                for c in self.classes.keys():\n",
    "                    classes_scores[c] += np.log((self.feats_counts[c, word] + self.alpha) / (\n",
    "                        self.classes[c] + self.alpha * len(self.vocab)))\n",
    "\n",
    "            for c in self.classes.keys():\n",
    "                classes_scores[c] += self.log_classes_freq[c]\n",
    "\n",
    "            preds.append(max(classes_scores.items(), key=lambda x: x[1])[0])\n",
    "\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7951680672268907\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer = FunctionTransformer(clean_texts)\n",
    "pipeline = Pipeline([('t', transformer),\n",
    "                     ('cls', Bayes(1))])\n",
    "\n",
    "df = read_csv(\"./train.csv\")\n",
    "\n",
    "x = df['text'].tolist()\n",
    "y = df['target'].tolist()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "pred = pipeline.predict(x_test)\n",
    "\n",
    "print('Accuracy =', accuracy_score(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7909663865546218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "transformer = FunctionTransformer(\n",
    "    lambda data: [\" \".join(clean_text(x, morph=morph)) for x in data])\n",
    "\n",
    "pipeline = Pipeline([('t', transformer),\n",
    "                     (\"vectorizer\", CountVectorizer()),\n",
    "                     ('cls', MultinomialNB(alpha=1))])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "pred = pipeline.predict(x_test)\n",
    "\n",
    "print('Accuracy =', accuracy_score(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание:\n",
    "1. Скачать датасет (https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis).\n",
    "2. Предобработать текст в колонке text (подумать, как именно это можно сделать).\n",
    "3. Разделить выборку на обучающую и тестовую, используя стратификацию.\n",
    "4. Создать пайплайн для обучения с использованием полиномиального наивного байесовского классификатора из библиотеки sklearn и различных подходов к векторизации текста (CountVectorizer, TfidfVectorizer) и т.д + сравнить их.\n",
    "5. Обучить модель и оценить кач-во с помощью метрик accuracy и f1-score."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
